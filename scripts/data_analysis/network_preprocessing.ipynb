{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose and use\n",
    "\n",
    "This Jupyter Notebook is to process the scraped revisions to construct network-ready dataframe.\n",
    "\n",
    "**The output**: Each record in the output **nodeEdge.csv** presents a one-to-one relationship between two articles. Each node represents an information source, and an edge between any two node (the row in nodeEdge) represents the interlinked relationship, which cues a potential trace of information creation.\n",
    "\n",
    "**Next step**: The output **nodeEdge** will be imported into Gephi for network visualisation and modularity computation.\n",
    "\n",
    "**Output data schema:**\n",
    "- Id: Revision ID, string\n",
    "- ParentId: Parent Revision ID, string\n",
    "- ArticleName: Article title, string\n",
    "- TimeStamp: Revision time, string\n",
    "- Link: An hyperlink in the revision text, string\n",
    "- LinkTitle: Hyperlink title, string\n",
    "- LinkType: Internal or external hyperlink, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from lxml import etree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_revision(revision, article_name):\n",
    "    # metadata\n",
    "    rev_id = revision.find('id').text if revision.find('id') is not None else None\n",
    "    parent_id = revision.find('parentid').text if revision.find('parentid') is not None else None\n",
    "    timestamp_str = revision.find('timestamp').text if revision.find('timestamp') is not None else None\n",
    "    timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '')) if timestamp_str else None\n",
    "    year, month, day = (timestamp.year, timestamp.month, timestamp.day) if timestamp else (None, None, None)\n",
    "\n",
    "    text = revision.find('text').text if revision.find('text') is not None else \"\"\n",
    "    if text:\n",
    "        internal_link_pattern = re.compile(r'\\[\\[([^\\]|]+)(?:\\|([^\\]]+))?\\]\\]')\n",
    "        external_link_pattern = re.compile(r'\\{\\{(cite\\s\\w+)\\s.*?url\\s*=\\s*([^|]+).*?title\\s*=\\s*([^|]+)')\n",
    "\n",
    "        links = []\n",
    "\n",
    "        # Process internal links\n",
    "        for match in internal_link_pattern.finditer(text):\n",
    "            link = \"https://en.wikipedia.org/wiki/\" + match.group(1).replace(' ', '_')\n",
    "            title = match.group(2) if match.group(2) else match.group(1)\n",
    "            link_type = \"internal\"\n",
    "            links.append((link.strip(), title.strip(), link_type))\n",
    "\n",
    "        # Process external links\n",
    "        for match in external_link_pattern.finditer(text):\n",
    "            link_type = match.group(1).strip()  # Get type after \"cite\"\n",
    "            link = match.group(2).strip()\n",
    "            title = match.group(3).strip()\n",
    "            links.append((link, title, link_type))\n",
    "        return [\n",
    "            {\n",
    "                \"Id\": rev_id,\n",
    "                \"ParentId\": parent_id,\n",
    "                \"ArticleName\": article_name,\n",
    "                \"TimeStamp\": timestamp,\n",
    "                \"Year\": year,\n",
    "                \"Month\": month,\n",
    "                \"Day\": day,\n",
    "                \"Link\": link,\n",
    "                \"LinkTitle\": title,\n",
    "                \"LinkType\": link_type\n",
    "            }\n",
    "            for link, title, link_type in links\n",
    "        ]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_all_xml_files(root_folder, article_name, file_name='nodeEdge.csv'):\n",
    "    folder = os.path.join(root_folder, article_name)\n",
    "    output_file = os.path.join(root_folder, file_name)\n",
    "    print(\"Crawling folder:\", folder)\n",
    "    file_mode = 'a' if os.path.exists(output_file) else 'w'\n",
    "    \n",
    "    with open(output_file, mode=file_mode, newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\n",
    "            \"Id\", \"ParentId\", \"ArticleName\", \"TimeStamp\", \"Year\", \"Month\", \"Day\", \"Link\", \"LinkTitle\", \"LinkType\"\n",
    "        ])\n",
    "        \n",
    "        if file_mode == 'w':\n",
    "            writer.writeheader()\n",
    "\n",
    "        pattern = os.path.join(folder, \"**\", \"*.xml\")\n",
    "        xml_files = glob.glob(pattern, recursive=True)\n",
    "        print(\"Found XML files:\", len(xml_files))\n",
    "        \n",
    "        for xml_file in xml_files:\n",
    "            try:\n",
    "                for event, elem in etree.iterparse(xml_file, tag='revision', events=('end',)):\n",
    "                    link_data = parse_revision(elem, article_name)\n",
    "                    for data in link_data:\n",
    "                        writer.writerow(data)\n",
    "                    elem.clear()\n",
    "                    while elem.getprevious() is not None:\n",
    "                        del elem.getparent()[0]\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {xml_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling folder: /Users/Administrator/Desktop/OII/FSDS24/Groupwork/wiki_project/data/Kanye West\n",
      "Found XML files: 10343\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.dirname(os.path.dirname(os.getcwd()))+os.sep+\"data\"\n",
    "kanye = \"Kanye West\"\n",
    "crawl_all_xml_files(folder_path, kanye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling folder: /Users/Administrator/Desktop/OII/FSDS24/Groupwork/wiki_project/data/Taylor Swift\n",
      "Found XML files: 7796\n"
     ]
    }
   ],
   "source": [
    "taylor = \"Taylor Swift\"\n",
    "crawl_all_xml_files(folder_path, taylor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
