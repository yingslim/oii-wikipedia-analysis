{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose and use\n",
    "\n",
    "This Jupyter Notebook is to process the scraped revisions to construct network-ready dataframe.\n",
    "\n",
    "**The output**: Each record in the output **nodeEdge.csv** presents a one-to-one relationship between two articles. Each node represents an information source, and an edge between any two node (the row in nodeEdge) represents the interlinked relationship, which cues a potential trace of information creation.\n",
    "\n",
    "**Next step**: The output **nodeEdge** will be imported into Gephi for network visualisation and modularity computation.\n",
    "\n",
    "**Output data schema:**\n",
    "- Id: Revision ID, string\n",
    "- ParentId: Parent Revision ID, string\n",
    "- ArticleName: Article title, string\n",
    "- TimeStamp: Revision time, string\n",
    "- Link: An hyperlink in the revision text, string\n",
    "- LinkTitle: Hyperlink title, string\n",
    "- LinkType: Internal or external hyperlink, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_index = {\"Taylor_Swift\": 1, \"Kanye_West\": 2}\n",
    "node_id_counter = 3\n",
    "edge_id_counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_data(nodes_file, edges_file):\n",
    "    global node_index, node_id_counter, edge_id_counter\n",
    "    if os.path.exists(nodes_file):\n",
    "        with open(nodes_file, mode='r', encoding='utf-8') as nodes_f:\n",
    "            reader = csv.DictReader(nodes_f)\n",
    "            for row in reader:\n",
    "                node_id = int(row[\"nodeId\"])\n",
    "                node_index[row[\"name\"]] = node_id\n",
    "                node_id_counter = max(node_id_counter, node_id + 1)\n",
    "\n",
    "    if os.path.exists(edges_file):\n",
    "        with open(edges_file, mode='r', encoding='utf-8') as edges_f:\n",
    "            reader = csv.DictReader(edges_f)\n",
    "            for row in reader:\n",
    "                edge_id = int(row[\"edgeId\"])\n",
    "                edge_id_counter = max(edge_id_counter, edge_id + 1)\n",
    "\n",
    "# Load existing data if files already exist\n",
    "load_existing_data('node.csv', 'edge.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_revision(revision, article_name):\n",
    "    # metadata\n",
    "    rev_id = revision.find('id').text if revision.find('id') is not None else None\n",
    "    parent_id = revision.find('parentid').text if revision.find('parentid') is not None else None\n",
    "    timestamp_str = revision.find('timestamp').text if revision.find('timestamp') is not None else None\n",
    "    timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '')) if timestamp_str else None\n",
    "    year, month, day = (timestamp.year, timestamp.month, timestamp.day) if timestamp else (None, None, None)\n",
    "\n",
    "    text = revision.find('text').text if revision.find('text') is not None else \"\"\n",
    "    if text:\n",
    "        internal_link_pattern = re.compile(r'\\[\\[([^\\]|]+)(?:\\|([^\\]]+))?\\]\\]')\n",
    "        external_link_pattern = re.compile(r'\\{\\{(cite\\s\\w+)\\s.*?url\\s*=\\s*([^|]+).*?title\\s*=\\s*([^|]+)')\n",
    "\n",
    "        links = []\n",
    "\n",
    "        # Process internal links\n",
    "        for match in internal_link_pattern.finditer(text):\n",
    "            link = \"https://en.wikipedia.org/wiki/\" + match.group(1).replace(' ', '_')\n",
    "            title = match.group(2) if match.group(2) else match.group(1)\n",
    "            link_type = \"internal\"\n",
    "            links.append((link.strip(), title.strip(), link_type))\n",
    "\n",
    "        # Process external links\n",
    "        for match in external_link_pattern.finditer(text):\n",
    "            link_type = match.group(1).strip()  # Get type after \"cite\"\n",
    "            link = match.group(2).strip()\n",
    "            title = match.group(3).strip()\n",
    "            links.append((link, title, link_type))\n",
    "        return [\n",
    "            {\n",
    "                \"revId\": rev_id,\n",
    "                \"ParentId\": parent_id,\n",
    "                \"ArticleName\": article_name,\n",
    "                \"TimeStamp\": timestamp,\n",
    "                \"Year\": year,\n",
    "                \"Month\": month,\n",
    "                \"Day\": day,\n",
    "                \"Link\": link,\n",
    "                \"LinkTitle\": title,\n",
    "                \"LinkType\": link_type\n",
    "            }\n",
    "            for link, title, link_type in links\n",
    "        ]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.read_csv(\"nodes_indexed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_all_xml_files(root_folder, article_name, nodes_file='node.csv', edges_file='edge.csv', nodeEdge_file='nodeEdge.csv'):\n",
    "    global node_index, node_id_counter, edge_id_counter\n",
    "    folder = os.path.join(root_folder, article_name)\n",
    "    print(\"Crawling folder:\", folder)\n",
    "    curr = \"Taylor_Swift\" if \"aylor\" in folder else \"Kanye_West\"\n",
    "\n",
    "    # Open node, edge, and nodeEdge files for appending\n",
    "    with open(nodes_file, mode='a', newline='', encoding='utf-8') as nodes_f, \\\n",
    "         open(edges_file, mode='a', newline='', encoding='utf-8') as edges_f, \\\n",
    "         open(nodeEdge_file, mode='a', newline='', encoding='utf-8') as nodeEdges_f:\n",
    "\n",
    "        node_writer = csv.DictWriter(nodes_f, fieldnames=[\"nodeId\", \"name\"])\n",
    "        edge_writer = csv.DictWriter(edges_f, fieldnames=[\"edgeId\",\"revId\", \"TimeStamp\", \"from\", \"to\", \"Year\", \"Month\",\"Day\",\"LinkType\"])\n",
    "        nodeEdge_writer = csv.DictWriter(nodeEdges_f, fieldnames=[\"edgeId\", \"revId\", \"ParentId\", \"ArticleName\", \"TimeStamp\", \"Year\", \"Month\", \"Day\", \"Link\", \"LinkTitle\", \"LinkType\"])\n",
    "\n",
    "        if nodes_f.tell() == 0:\n",
    "            node_writer.writeheader()\n",
    "            node_writer.writerow({\"nodeId\": 1, \"name\": \"Taylor_Swift\"})\n",
    "            node_writer.writerow({\"nodeId\": 2, \"name\": \"Kanye_West\"})\n",
    "        if edges_f.tell() == 0:\n",
    "            edge_writer.writeheader()\n",
    "        if nodeEdges_f.tell() == 0:\n",
    "            nodeEdge_writer.writeheader()\n",
    "\n",
    "        pattern = os.path.join(folder, \"**\", \"*.xml\")\n",
    "        xml_files = glob.glob(pattern, recursive=True)\n",
    "        print(\"Found XML files:\", len(xml_files))\n",
    "\n",
    "        for xml_file in xml_files:\n",
    "            try:\n",
    "                for event, elem in etree.iterparse(xml_file, tag='revision', events=('end',)):\n",
    "                    link_data = parse_revision(elem, article_name)\n",
    "\n",
    "                    for data in link_data:\n",
    "                        source_name = curr\n",
    "                        target_name = data[\"LinkTitle\"]\n",
    "                        if source_name not in node_index:\n",
    "                            node_index[source_name] = node_id_counter\n",
    "                            node_writer.writerow({\"nodeId\": node_index[source_name], \"name\": source_name})\n",
    "                            node_id_counter += 1\n",
    "                        \n",
    "                        if target_name not in node_index:\n",
    "                            node_index[target_name] = node_id_counter\n",
    "                            node_writer.writerow({\"nodeId\": node_index[target_name], \"name\": target_name})\n",
    "                            node_id_counter += 1\n",
    "\n",
    "                        edge_writer.writerow({\n",
    "                            \"edgeId\": edge_id_counter,\n",
    "                            \"TimeStamp\": data[\"TimeStamp\"],\n",
    "                            \"revId\": data[\"revId\"],\n",
    "                            \"from\": node_index[source_name],\n",
    "                            \"to\": node_index[target_name],\n",
    "                            \"Year\": data[\"Year\"],\n",
    "                            \"Month\": data[\"Month\"],\n",
    "                            \"Day\": data[\"Day\"],\n",
    "                            \"LinkType\": data[\"LinkType\"]\n",
    "                        })\n",
    "\n",
    "                        nodeEdge_writer.writerow({\n",
    "                            \"edgeId\": edge_id_counter,\n",
    "                            \"revId\": data[\"revId\"],\n",
    "                            \"ParentId\": data[\"ParentId\"],\n",
    "                            \"ArticleName\": data[\"ArticleName\"],\n",
    "                            \"TimeStamp\": data[\"TimeStamp\"],\n",
    "                            \"Year\": data[\"Year\"],\n",
    "                            \"Month\": data[\"Month\"],\n",
    "                            \"Day\": data[\"Day\"],\n",
    "                            \"Link\": data[\"Link\"],\n",
    "                            \"LinkTitle\": data[\"LinkTitle\"],\n",
    "                            \"LinkType\": data[\"LinkType\"]\n",
    "                        })\n",
    "                        edge_id_counter += 1\n",
    "\n",
    "                    elem.clear()\n",
    "                    while elem.getprevious() is not None:\n",
    "                        del elem.getparent()[0]\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {xml_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling folder: /Users/Administrator/Desktop/OII/FSDS24/Groupwork/wiki_project/data/Kanye_West\n",
      "Found XML files: 9642\n",
      "Crawling folder: /Users/Administrator/Desktop/OII/FSDS24/Groupwork/wiki_project/data/Taylor_Swift\n",
      "Found XML files: 19301\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.dirname(os.path.dirname(os.getcwd())) + os.sep + \"data\"\n",
    "crawl_all_xml_files(folder_path, \"Kanye_West\")\n",
    "crawl_all_xml_files(folder_path, \"Taylor_Swift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming columns name\n",
    "Transforming the column names for Gephi ready schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"node.csv\")\n",
    "df.rename(columns={\"name\": \"Label\",\"nodeId\":\"Id\"}, inplace=True)\n",
    "df.to_csv(\"node.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"edge.csv\")\n",
    "df.rename(columns={\"from\": \"source\", \"to\": \"target\"}, inplace=True)\n",
    "df.to_csv(\"edge.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_index_df = pd.DataFrame(node_index, index=[0])\n",
    "transformed_df = pd.DataFrame({\n",
    "    \"Id\": node_index_df.iloc[0].values,\n",
    "    \"Label\": node_index_df.columns\n",
    "})\n",
    "transformed_df.to_csv(\"nodes_indexed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated edges saved to TK_Edge_2012-07-01.csv\n"
     ]
    }
   ],
   "source": [
    "def filter_and_aggregate_edges(edges_file, filterOffsetDate):\n",
    "    cutoff_date = datetime.strptime(filterOffsetDate, '%Y-%m-%d')\n",
    "    df = pd.read_csv(edges_file)\n",
    "\n",
    "    df['TimeStamp'] = pd.to_datetime(df['TimeStamp'])\n",
    "    filtered_df = df[df['TimeStamp'] <= cutoff_date]\n",
    "\n",
    "    aggregated_df = filtered_df.groupby(['source', 'target'], as_index=False).agg({\n",
    "        'edgeId': 'first',\n",
    "        'revId': 'count',\n",
    "        'Year': 'first',\n",
    "        'Month': 'first',\n",
    "        'Day': 'first',\n",
    "        'LinkType': 'first',\n",
    "    })\n",
    "\n",
    "    aggregated_df.rename(columns={'revId': 'weight'}, inplace=True)\n",
    "\n",
    "    output_file = f\"TK_Edge_{filterOffsetDate}.csv\"\n",
    "    aggregated_df.to_csv(output_file, index=False)\n",
    "    print(f\"Aggregated edges saved to {output_file}\")\n",
    "\n",
    "\n",
    "filter_and_aggregate_edges(edges_file=\"edge.csv\", filterOffsetDate=\"2012-07-01\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for filtering common\n",
    "\n",
    "    Filters nodes that have edges connecting to both \"Taylor Swift\" and \"Kanye West\" for the first time after the specified date and aggregates the edges by adding a weight.\n",
    "\n",
    "    Args:\n",
    "    - edges_file (str): The path to the edge.csv file.\n",
    "    - filterOffsetDate (str): The cutoff date in the format 'YYYY-MM-DD'.\n",
    "    - A CSV file named TK_NewlyConnectedNodes_[filterOffsetDate].csv containing the filtered and aggregated edges.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and aggregated edges saved to TK_NewlyConnectedNodes_2012-05-01.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def filter_newly_connected_nodes(edges_file, filterOffsetDate):\n",
    "    cutoff_date = datetime.strptime(filterOffsetDate, '%Y-%m-%d')\n",
    "    df = pd.read_csv(edges_file)\n",
    "    df['TimeStamp'] = pd.to_datetime(df['TimeStamp'])\n",
    "\n",
    "    taylor_id = 1\n",
    "    kanye_id = 2\n",
    "\n",
    "    df = df.sort_values(by='TimeStamp')\n",
    "    first_appearance_df = df.groupby(['source', 'target'], as_index=False).first()\n",
    "    filtered_df = first_appearance_df[first_appearance_df['TimeStamp'] > cutoff_date]\n",
    "    relevant_nodes = filtered_df.groupby('target').filter(lambda x: {taylor_id, kanye_id}.issubset(x['source'].values))\n",
    "    relevant_edges = df[df['target'].isin(relevant_nodes['target']) & (df['TimeStamp'] > cutoff_date)]\n",
    "    aggregated_edges = relevant_edges.groupby(['source', 'target'], as_index=False).agg({\n",
    "        'TimeStamp': 'first',\n",
    "        \"Year\":\"first\",\n",
    "        \"Month\":\"first\",\n",
    "        'edgeId': 'first',\n",
    "        'LinkType': 'first',\n",
    "        'revId': 'count',\n",
    "    }).rename(columns={'revId': 'weight', \"year\":\"first_appeared_in_year\",\"month\":\"first_appeared_in_month\"})\n",
    "\n",
    "    output_file = f\"TK_NewlyConnectedNodes_{filterOffsetDate}.csv\"\n",
    "    aggregated_edges.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered and aggregated edges saved to {output_file}\")\n",
    "\n",
    "filter_newly_connected_nodes(edges_file=\"edge.csv\", filterOffsetDate=\"2006-12-30\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function: Getting comparative datasets and unique newly emerged dataset\n",
    "\n",
    "    Generates two CSV files:\n",
    "    1. All unique edges with weights before the cutoff date, including target names.\n",
    "    2. New edges formed within the specified day range after the cutoff date, excluding edges that appeared before.\n",
    "\n",
    "    Args:\n",
    "    - edges_file (str): The path to the edge.csv file.\n",
    "    - cutoff_date (str): The cutoff date in the format 'YYYY-MM-DD'.\n",
    "    - day_range (int): Number of days after the cutoff date to capture new edges.\n",
    "\n",
    "    Outputs:\n",
    "    - CSV file `UniqueEdgesBefore_[cutoff_date].csv` for edges before the cutoff date.\n",
    "    - CSV file `NewEdgesWithinRange_[cutoff_date]_Range_[day_range].csv` for new edges in the specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique edges before 2009-09-11 saved to UniqueEdgesBefore_2009-09-11.csv\n",
      "New edges within 30 days after 2009-09-11 saved to NewEdgesWithinRange_2009-09-11_Range_30.csv\n"
     ]
    }
   ],
   "source": [
    "nodes_df = pd.read_csv(\"nodes_indexed.csv\") \n",
    "id_to_label = dict(zip(nodes_df['Id'], nodes_df['Label']))\n",
    "\n",
    "\n",
    "def generate_edge_csvs(edges_file, cutoff_date, day_range):\n",
    "    cutoff_datetime = datetime.strptime(cutoff_date, '%Y-%m-%d')\n",
    "    range_end_date = cutoff_datetime + timedelta(days=day_range)\n",
    "    df = pd.read_csv(edges_file)\n",
    "    df['TimeStamp'] = pd.to_datetime(df['TimeStamp'])\n",
    "    df['target_name'] = df['target'].map(id_to_label)\n",
    "\n",
    "    edges_before_cutoff = df[df['TimeStamp'] < cutoff_datetime]\n",
    "    unique_edges_before = edges_before_cutoff.groupby(['source', 'target', 'target_name'], as_index=False).agg({\n",
    "        'TimeStamp': 'first', \n",
    "        'edgeId': 'first', \n",
    "        'LinkType': 'first',  \n",
    "        'revId': 'count', \n",
    "    }).rename(columns={'revId': 'weight'})\n",
    "\n",
    "    # Save unique edges before the cutoff date to CSV\n",
    "    output_file_before = f\"UniqueEdgesBefore_{cutoff_date}.csv\"\n",
    "    unique_edges_before.to_csv(output_file_before, index=False)\n",
    "    print(f\"Unique edges before {cutoff_date} saved to {output_file_before}\")\n",
    "    existing_edges = set(zip(unique_edges_before['source'], unique_edges_before['target']))\n",
    "    edges_after_cutoff = df[(df['TimeStamp'] >= cutoff_datetime) & (df['TimeStamp'] <= range_end_date)]\n",
    "\n",
    "    # Exclude any edge that appeared in the prior dataset\n",
    "    new_edges_in_range = edges_after_cutoff[~edges_after_cutoff[['source', 'target']].apply(tuple, axis=1).isin(existing_edges)]\n",
    "    aggregated_new_edges = new_edges_in_range.groupby(['source', 'target', 'target_name'], as_index=False).agg({\n",
    "        'TimeStamp': 'first',\n",
    "        'edgeId': 'first',\n",
    "        'LinkType': 'first',\n",
    "        'revId': 'count',\n",
    "    }).rename(columns={'revId': 'weight'})\n",
    "\n",
    "    # Save new edges within the specified day range to CSV\n",
    "    output_file_after = f\"NewEdgesWithinRange_{cutoff_date}_Range_{day_range}.csv\"\n",
    "    aggregated_new_edges.to_csv(output_file_after, index=False)\n",
    "    print(f\"New edges within {day_range} days after {cutoff_date} saved to {output_file_after}\")\n",
    "\n",
    "generate_edge_csvs(edges_file=\"edge.csv\", cutoff_date=\"2009-09-11\", day_range=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
